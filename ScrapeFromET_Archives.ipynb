{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "224b2c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://economictimes.indiatimes.com/archivelist/year-2019,month-5,starttime-43592.cms\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Find all the headings\n",
    "    headings = soup.find_all(\"h3\")\n",
    "    \n",
    "    # Extract and print the text of each heading\n",
    "    for heading in headings:\n",
    "        print(heading.text.strip())\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd2b4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines found: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://economictimes.indiatimes.com/archivelist/year-2019,month-5,starttime-43592.cms\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Debugging: Print the HTML content to see if it's fetched correctly\n",
    "    # print(soup.prettify())\n",
    "    \n",
    "    # Find all the headline elements\n",
    "    headlines = soup.find_all(\"span\", class_=\"listheadline\")\n",
    "    \n",
    "    # Debugging: Print the number of headline elements found\n",
    "    print(\"Number of headlines found:\", len(headlines))\n",
    "    \n",
    "    # Extract and print the text of each headline\n",
    "    for headline in headlines:\n",
    "        print(headline.text.strip())\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6425a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.17.2-py3-none-any.whl (9.9 MB)\n",
      "     ---------------------------------------- 9.9/9.9 MB 6.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
      "     -------------------------------------- 460.2/460.2 kB 5.8 MB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Collecting sniffio>=1.3.0\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sniffio, outcome, h11, exceptiongroup, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed exceptiongroup-1.2.0 h11-0.14.0 outcome-1.3.0.post0 selenium-4.17.2 sniffio-1.3.0 trio-0.24.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d416a93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics\n",
      "1/8th of flora & fauna face extinction due to degradation of biodiversity\n",
      "1/8th of flora & fauna face extinction due to degradation of biodiversity\n",
      "They like it hot! Some companies have a summer to cheer\n",
      "Dismal voter turnout continues in Kashmir with 2.81% in Anantnag; 63% in Ladakh\n",
      "Men ‘fearful’ of Maoists, police ‘desert’ Gadchiroli village\n",
      "India’s consumption story losing the plot\n",
      "Pramit Jhaveri to lead Citi’s Asia Pacific operations?\n",
      "Polling over in 424 seats, spotlight on eastern UP\n",
      "9 Indian stocks have the most number of sell calls across the globe\n",
      "Trump tweet spooks global markets, Sensex falls 362 pts, Nifty below 11,600\n",
      "Niti Aayog working on proposal to appraise judges’ performance, make rankings public\n",
      "Appellate body stays Sebi ban on 3 NSE employees\n",
      "Standard Life sells 1.78% in HDFC Life Insurance\n",
      "Chinese stocks and currency markets take it on the chin\n",
      "Market Movers: What changed for D-Street while you were sleeping\n",
      "Gold may regain safe haven status for investors this Akshaya Tritiya\n",
      "ITAT ruling on ‘stock in trade’ may lead to spate of litigations\n",
      "'BUY' or 'SELL' ideas from experts for Tuesday 7 May 2019\n",
      "State Bank of India shortlists Deloitte as Interim Resolution Professional to manage RCom\n",
      "Base metals, edible oil may decline\n",
      "Maharashtra mills set yet another record in sugar output\n",
      "India may reject US demand for outright ban on Huawei\n",
      "Maize shortage hits poultry feed, starch industries\n",
      "Importance of localising data in payments ecosystem\n",
      "Buzzing stocks: RCom, PC Jeweller,ICICI Bank, RPower, Bharti Airtel\n",
      "Supreme Court to hear plea to set time limit for EC to decide poll code breaches\n",
      "Marico, Sterlite Tech among top gainers on BSE\n",
      "India, Germany, Brazil, Japan 'absolutely needed' at UNSC as permanent members: France\n",
      "Mangalam Seeds, GTN Industries among top losers on BSE\n",
      "Zero vote cast from Burhan Wani's village, only 15 from Pulwama suicide bomber's\n",
      "EC dissensions good, must be highlighted: Abhishek Singhvi\n",
      "Share market update: Auto shares in the green; TVS Motor up 2%\n",
      "Sensex climbs over 150 pts, Nifty above 11,650; Asian markets take a breather\n",
      "Bullet-proofing experts, slogan writers win big in India polls\n",
      "Afghan peace talks, US troops on India's agenda\n",
      "Share market update: PSU Bank shares gain; Syndicate Bank rises 2%\n",
      "Need to build bridges between industry and science: Tarun Khanna\n",
      "Share market update: IT shares rise; Tata Elxsi gains over 2%\n",
      "EVM tech prone to manipulation: Sam Pitroda\n",
      "Share market update: Nifty Pharma index up; Biocon rises 1%\n",
      "Poll code violations: Most complaints from UP\n",
      "Cognizant: Mighty growth engine comes to near halt\n",
      "Share market update: Realty shares slip; Sobha down over 1%\n",
      "BJP will need help of allies to form government: Ram Madhav\n",
      "Share market update: 8 stocks hit 52-week highs on NSE\n",
      "Army to induct 460 Russian-origin tanks to add muscle on Pakistan front\n",
      "Share market update: Metal shares advance; NMDC, Vedanta among top gainers\n",
      "Building Heavyweight Banks: Economies of scale in bank mergers can be huge\n",
      "Share market update: FMCG shares trade higher; Marico surges 7%\n",
      "IAF beyond Balakot: Time to rise above political differences\n",
      "Tata Steel, HUL, Bajaj Auto, Titan among 36 stocks that look set to fall, shows MACD\n",
      "Most SC judges rally behind CJI Gogoi, question Justice Chandrachud’s letter to inquiry panel\n",
      "Share market update: Bank shares bullish; SBI, HDFC Bank among top gainers\n",
      "Gold gains as risk appetite dips on Trump tariff-hike threat\n",
      "Frame counting rules if VVPATs, EVMs don’t tally: Opposition parties to EC\n",
      "Share market update: 58 stocks hit 52-week lows on NSE\n",
      "Share market update: OMCs trade mixed; HPCL slumps over 4%\n",
      "Top intraday trading ideas for afternoon trade for Tuesday 7 May 2019\n",
      "Trump’s tariff tweets do the markets a big favor: Robert Burgess\n",
      "Mahatma Gandhi should be honoured with Congressional Gold Medal this year: US lawmaker\n",
      "Share market update: BSE Power index up; Power Grid gains over 1%\n",
      "IMG Reliance facilitates deals over Rs 150 crore this IPL\n",
      "Share market update: Consumer Durables shares trade higher; Blue Star jumps 5%\n",
      "Look where HNIs are parking money amid equity, debt market uncertainty\n",
      "Supreme Court allows Karti Chidambaram to travel abroad in May\n",
      "States will continue to run large deficits even after introduction of GST: Report\n",
      "Share market update: Telecom shares mixed; Tejas Networks dips over 2%\n",
      "Gold Rate Today: Gold prices little changed amid tepid spot demand\n",
      "Share market update: BSE Capital Goods index rises; Dilip Buildcon climbs nearly 5%\n",
      "Commodity outlook: Bullion counter may trade with an upside bias\n",
      "Share market update: Private Bank shares gain; RBL Bank rises 1%\n",
      "Share market update: Media shares mixed; Dish TV slips over 3%\n",
      "Billionaire Mittal's Airtel falters in war with Ambani's Jio\n",
      "Share market update: Nifty Financial Services index flat; Indiabulls Ventures dips nearly 3%\n",
      "Lanka now safe, all extremists linked to Easter bombings killed or arrested: Security authorities\n",
      "United Front-type govt \"only likelihood\" now: TRS leader\n",
      "Kya lagta hai? Wild swings ahead as D-Street assigns probabilities to BJP seat wins\n",
      "Polling officer dies, 24 injured as vehicle overturns in Amethi\n",
      "GMR to raise USD 350 million via bonds for Delhi airport\n",
      "Asian shares off 5-week low, remain fragile amid US-China trade tensions\n",
      "Nikkei slides on jitters over US-China trade, Sony soars\n",
      "Does EPFO really have enough money left for the year after IL&FS? FinMin wants to know\n",
      "Australian shares end higher on iron ore miners; NZ up\n",
      "RBI plans more FX swaps, steps to inject cash-officials\n",
      "Market expecting more gold buying by the uber rich this Akshaya Tritiya\n",
      "Delhi HC quashes FIR accusing AAP MLA Somnath Bharti of domestic violence\n",
      "Share market update: Sugar stocks fall; Balrampur Chini Mills slips 4%\n",
      "South India got step-motherly treatment, will play key role in voting out Modi: Shashi Tharoor\n",
      "Two students of Army's 'Samba Super-40' cleared AFCAT exam\n",
      "Eros International tries new funding plan\n",
      "Jalan Panel bats for lowering of RBI reserves\n",
      "Essel group stocks tumble up to 11%\n",
      "After surprise Q4 results, Airtel keeps Street guessing on KPIs, stock slips\n",
      "200 DU teachers issue statement condemning Modi's remarks on former PM Rajiv Gandhi\n",
      "Congress will protect forests, land and water of tribals: Rahul Gandhi\n",
      "Women among 55 protesters detained for staging protest outside SC\n",
      "Fitch revises Lodha's outlook to negative from stable\n",
      "US commerce secretary warns India on 'unfair' trade policies\n",
      "ED attaches 'bribe' cash seized from former rail min Pawan Bansal's nephew\n",
      "There is no divide between Jats and non-Jats: Deepender Hooda, Congress’ three-term Rohtak MP\n",
      "LS polls: 'Baba Ji Burger Wale', 'Chacha Maggi Wala' in fray in Punjab\n",
      "Priyanka attacks Modi, says Duryodhana too had such arrogance\n",
      "SC in-house committee report clearing CJI should be made public: Ex-Information Commissioner\n",
      "Meeting with Telangana CM \"a significant one\": Pinarayi Vijayan\n",
      "Tech View: Nifty breaks trading range, stands at make-or-break point\n",
      "CJI case: Complainant seeks in-house panel report from members\n",
      "Will continue to fight for transparency in election process: Chandrababu Naidu after setback in Supreme Court\n",
      "Opposition against triple talaq which gives dignity to Muslim women: Yogi Adityanath\n",
      "Government can expel any foreigner from India without a show cause: MHA to High Court\n",
      "After Market: Airtel drops despite good Q4, ICICI Bank slips; 201 stocks hit lower circuits\n",
      "India, China to have highest health benefits from reduced emissions: Study\n",
      "Election Commission acted against others, but not Narendra Modi, Amit Shah, Congress tells Supreme Court\n",
      "Retd IAF man joins Congress, says PM Modi politicising military's work\n",
      "Why Jyotiraditya Scindia campaigns with an onion in his pocket\n",
      "Hong Kong may set up office in India\n",
      "India's gold imports likely jumped ahead of Akshaya Tritiya\n",
      "Suresh Prabhu for govt-to-govt agreement between India and US to facilitate private sector firms\n",
      "Essar Steel shareholder seeks rejection of ArcelorMittal bid for firm\n",
      "CoCs must share all info on NPA accounts with bidders: IBBI\n",
      "Zee group, Anil Ambani firms lead in share pledges with lenders\n",
      "Former head of MSCI India Chandru Badrinarayanan joins ECube\n",
      "Jalan panel to hold at least 2 more meets before final report on RBI capital size\n",
      "Gold finally sparkles this Akshaya Tritiya, sales up over 25%\n",
      "Nirav Modi set for fresh bail plea in UK court\n",
      "Met department issues 'yellow' weather warning for thunderstorm in Himachal Pradesh\n",
      "Maharashtra gets Rs 2,160 crore more from Centre for drought relief\n",
      "US may take final decision on GSP issue after formation of new government\n",
      "What has Congress done to eradicate poverty: Nitin Gadkari\n",
      "Electrosteel Steels acquisition led to a turnaround in firm: Vedanta\n",
      "Indian businesses in Africa suggest steps to increase trade, investment\n",
      "Trade setup: Nifty may continue bearish trend till it tops 11,550\n",
      "Arun Jaitley calls Congress party 'cry baby', says MCC cannot encroach right to free speech\n",
      "Yes Bank acquires eight crore pledged shares of CG Power\n",
      "India has made many important contributions to Afghanistan's development: US\n",
      "PM Modi changing goal-posts, is a stuntman and not statesman: Congress\n",
      "EVMs found in Bihar hotel, four cops suspended, poll official issued show cause notice\n",
      "Stake sales process of Zee at an advanced stage: Essel Group\n",
      "Terrorism now limted to Kashmir due to Modi's actions: Rajnath Singh\n",
      "Bofors petitioner and former BJP leader campaigns against Modi in Varanasi\n",
      "Antonio Guterres lauds India's continued support to UN's counter-terrorism work\n",
      "Narendra Modi is workaholic, Rahul loves holidays: Amit Shah\n",
      "Rahul Gandhi seeks time till weekend to respond to EC show-cause\n",
      "Johnson & Johnson agrees to pay about $1 billion to resolve hip implant lawsuits\n",
      "Fani declared ‘Extremely Severe’ natural calamity\n",
      "Donald Trump ignoring subpoenas can be impeachable offence: Nancy Pelosi\n",
      "Bonus share issues boom in show of corporate strength\n",
      "More »\n",
      "Bharat Bandh Live\n",
      "CTET Result\n",
      "PM Modi UAE Visit\n",
      "EPFO Interest Rate Hike\n",
      "Parliament Budget Session Live\n",
      "RBI MPC Meet Live\n",
      "Kerala Budget 2024\n",
      "Budget 2024 LIVE\n",
      "Budget 2024 Highlights\n",
      "Key Numbers\n",
      "Common Man\n",
      "Key takeaways\n",
      "Bahi khata\n",
      "Key Announcements\n",
      "Budget at a Glance\n",
      "Bharat Bandh Live\n",
      "CTET Result\n",
      "PM Modi UAE Visit\n",
      "EPFO Interest Rate Hike\n",
      "Parliament Budget Session Live\n",
      "Pakistan Election Results\n",
      "Noida Traffic Jam\n",
      "RBI MPC Meet Live\n",
      "ICAI CA Foundation Result 2023\n",
      "Uttarakhand UCC Bill Updates\n",
      "Kerala Budget 2024\n",
      "CBSE Admit Card 2024\n",
      "Budget 2024 LIVE\n",
      "Budget 2024 Highlights\n",
      "Key Numbers\n",
      "Bharat Bandh Live\n",
      "CTET Result\n",
      "PM Modi UAE Visit\n",
      "EPFO Interest Rate Hike\n",
      "Parliament Budget Session Live\n",
      "RBI MPC Meet Live\n",
      "Kerala Budget 2024 Highlights\n",
      "Budget 2024 LIVE\n",
      "Key Numbers\n",
      "Common Man\n",
      "Key takeaways\n",
      "Bahi khata\n",
      "Key Announcements\n",
      "Budget at a Glance\n",
      "Standard Deviation Calculator\n",
      "Age Calculator\n",
      "Time Calculator\n",
      "GPA Calculator\n",
      "Statistics Calculator\n",
      "Fraction Calculator\n",
      "Date Calculator\n",
      "Log Calculator\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://economictimes.indiatimes.com/archivelist/year-2019,month-5,starttime-43592.cms\"\n",
    "\n",
    "# Initialize the Selenium webdriver (make sure you have chromedriver installed and its path set)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the webpage\n",
    "driver.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all the <a> tags containing headlines\n",
    "headlines = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Extract and print the text of each headline\n",
    "for headline in headlines:\n",
    "    # Filter out headlines with certain URLs (adjust this condition as needed)\n",
    "    if \"/news/\" in headline['href']:\n",
    "        print(headline.text.strip())\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a191fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to D:\\MIT PGDAML\\Neural Network\\Project\\Econ_Times_Archives.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://economictimes.indiatimes.com/archivelist/year-2019,month-5,starttime-43592.cms\"\n",
    "\n",
    "# Initialize the Selenium webdriver (make sure you have chromedriver installed and its path set)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the webpage\n",
    "driver.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all the <a> tags containing headlines\n",
    "headlines = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Initialize an empty list to store headlines and dates\n",
    "data = []\n",
    "\n",
    "# Extract and store the text of each headline along with the date\n",
    "for headline in headlines:\n",
    "    # Filter out headlines with certain URLs (adjust this condition as needed)\n",
    "    if \"/news/\" in headline['href']:\n",
    "        # Get the date from the URL\n",
    "        date_str = url.split('starttime-')[1].split('.')[0]\n",
    "        date = datetime.fromordinal(int(date_str)).strftime(\"%Y-%m-%d\")\n",
    "        data.append((headline.text.strip(), date))\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "# Create an Excel workbook\n",
    "wb = Workbook()\n",
    "\n",
    "# Select the active worksheet\n",
    "ws = wb.active\n",
    "\n",
    "# Set the column headers\n",
    "ws.append([\"Headline\", \"Date\"])\n",
    "\n",
    "# Write the data to the worksheet\n",
    "for headline, date in data:\n",
    "    ws.append([headline, date])\n",
    "\n",
    "# Save the workbook\n",
    "file_path = r\"D:\\MIT PGDAML\\Neural Network\\Project\\Econ_Times_Archives.xlsx\"\n",
    "wb.save(file_path)\n",
    "\n",
    "print(\"Data has been successfully written to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a32e21ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to D:\\MIT PGDAML\\Neural Network\\Project\\Econ_Times_Archives.xlsx\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "import requests\n",
    "\n",
    "# URL of the archive page\n",
    "url = \"https://economictimes.indiatimes.com/archivelist/year-2018,month-1,starttime-43101.cms\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the headline elements\n",
    "headlines = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Create an Excel workbook\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Headline\"])  # Add column headers\n",
    "\n",
    "# Extract and write the text of each headline to the Excel file\n",
    "for headline in headlines:\n",
    "    # Check if the link corresponds to headlines\n",
    "    if \"/wealth/plan/\" in headline['href']:  # Adjust this condition according to the structure of the headlines links\n",
    "        headline_text = headline.text.strip()\n",
    "        ws.append([headline_text])\n",
    "\n",
    "# Save the workbook\n",
    "file_path = r\"D:\\MIT PGDAML\\Neural Network\\Project\\Econ_Times_Archives.xlsx\"\n",
    "wb.save(file_path)\n",
    "\n",
    "print(\"Data has been successfully written to\", file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7a921e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to D:\\MIT PGDAML\\Neural Network\\Project\\Econ_Times_Archives.xlsx\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import load_workbook, Workbook\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import calendar\n",
    "\n",
    "# Create or load existing Excel workbook\n",
    "file_path = r\"D:\\MIT PGDAML\\Neural Network\\Project\\Econ_Times_Archives.xlsx\"\n",
    "try:\n",
    "    wb = load_workbook(filename=file_path)\n",
    "    ws = wb.active\n",
    "except FileNotFoundError:\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Headline\", \"Date\"])  # Add column headers\n",
    "\n",
    "# Function to extract headlines and corresponding dates from a given URL\n",
    "def extract_headlines(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the headline elements\n",
    "    headlines = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    # Extract and write the text of each headline to the Excel file along with the corresponding date\n",
    "    for headline in headlines:\n",
    "        # Extract the headline text\n",
    "        headline_text = headline.text.strip()\n",
    "        # Check if the headline text exists (not empty)\n",
    "        if headline_text:\n",
    "            # Extract date from the URL\n",
    "            date_str = url.split('starttime-')[1].split('.')[0]\n",
    "            date = (datetime.fromordinal(int(date_str)) + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "            ws.append([headline_text, date])\n",
    "\n",
    "# Iterate through each day from 2018 to 2024 Jan and extract headlines\n",
    "for year in range(2018, 2025):\n",
    "    for month in range(1, 13):\n",
    "        # Determine the number of days in the month\n",
    "        num_days = calendar.monthrange(year, month)[1]\n",
    "        for day in range(1, num_days + 1):\n",
    "            # Construct URL for each day\n",
    "            url = f\"https://economictimes.indiatimes.com/archivelist/year-{year},month-{month},starttime-{datetime(year, month, day).toordinal()}.cms\"\n",
    "            extract_headlines(url)\n",
    "            time.sleep(1)  # Add a delay of 1 second between requests\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(file_path)\n",
    "print(\"Data has been successfully written to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df5ba4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize the Selenium webdriver (make sure you have chromedriver installed and its path set)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Create a new Excel workbook\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Headline\", \"Date\"])  # Add column headers\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2022, 1, 1)\n",
    "end_date = datetime(2022, 1, 10)\n",
    "\n",
    "# Loop through each day and extract headlines\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    # Construct the URL for the current day\n",
    "    url = f\"https://economictimes.indiatimes.com/archivelist/year-{current_date.year},month-{current_date.month},starttime-{current_date.toordinal()}.cms\"\n",
    "    \n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Find the footer section\n",
    "    footer = soup.find(\"footer\")\n",
    "\n",
    "    # Find all the <a> tags containing headlines outside the footer section\n",
    "    headlines = [headline for headline in soup.find_all(\"a\", href=True) if headline.find_parents(\"footer\") is None]\n",
    "\n",
    "    # Extract and write the text of each headline along with the date to the Excel file\n",
    "    for headline in headlines:\n",
    "        # Filter out headlines with certain URLs and exclude 'More »' section\n",
    "        if \"/news/\" in headline['href'] and 'More »' not in headline.text.strip():\n",
    "            ws.append([headline.text.strip(), current_date.strftime(\"%Y-%m-%d\")])\n",
    "\n",
    "    # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Save the workbook to the specified path\n",
    "file_path = r\"D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\"\n",
    "wb.save(file_path)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data has been successfully written to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec35d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "\n",
    "url = \"https://economictimes.indiatimes.com/archivelist/year-2019,month-5,starttime-43592.cms\"\n",
    "\n",
    "# Initialize the Selenium webdriver (make sure you have chromedriver installed and its path set)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the webpage\n",
    "driver.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all the <a> tags containing headlines\n",
    "headlines = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Create a new Excel workbook\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Headline\"])  # Add column header\n",
    "\n",
    "# Extract and write the text of each headline to the Excel file\n",
    "for headline in headlines:\n",
    "    # Filter out headlines with certain URLs (adjust this condition as needed)\n",
    "    if \"/news/\" in headline['href']:\n",
    "        ws.append([headline.text.strip()])\n",
    "\n",
    "# Save the workbook to the specified path\n",
    "file_path = r\"D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\"\n",
    "wb.save(file_path)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data has been successfully written to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b409c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "url = \"https://economictimes.indiatimes.com/archivelist/year-2019,month-5,starttime-43592.cms\"\n",
    "\n",
    "# Initialize the Selenium webdriver (make sure you have chromedriver installed and its path set)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the webpage\n",
    "driver.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Find all the <a> tags containing headlines\n",
    "headlines = soup.find_all(\"a\", href=True)\n",
    "\n",
    "# Extract and print the text of each headline\n",
    "headlines_text = []\n",
    "for headline in headlines:\n",
    "    # Filter out headlines with certain URLs (adjust this condition as needed)\n",
    "    if \"/news/\" in headline['href']:\n",
    "        headlines_text.append(headline.text.strip())\n",
    "\n",
    "# Remove the last 52 rows\n",
    "if len(headlines_text) > 52:\n",
    "    headlines_text = headlines_text[:-52]\n",
    "\n",
    "# Extract date from the URL\n",
    "url_date_str = url.split('starttime-')[1].split('.')[0]\n",
    "url_date = datetime(1899, 12, 30) + timedelta(days=int(url_date_str))\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "# Write the headlines and corresponding dates to the Excel file\n",
    "file_path = r\"D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\"\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.append([\"Headline\", \"Date\"])  # Add column headers\n",
    "for headline in headlines_text:\n",
    "    ws.append([headline, url_date.strftime(\"%d/%b/%Y\")])\n",
    "wb.save(file_path)\n",
    "\n",
    "print(\"Data has been successfully written to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "279e5a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize the Selenium webdriver (make sure you have chromedriver installed and its path set)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Create or load existing Excel workbook\n",
    "file_path = r\"D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\"\n",
    "try:\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Headline\", \"Date\"])  # Add column headers\n",
    "except FileNotFoundError:\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Headline\", \"Date\"])  # Add column headers\n",
    "\n",
    "# Function to extract headlines and corresponding dates from a given URL\n",
    "def extract_headlines(url, ws):\n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Find all the <a> tags containing headlines\n",
    "    headlines = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    # Extract and write the text of each headline along with the corresponding date\n",
    "    headlines_text = []\n",
    "    for headline in headlines:\n",
    "        # Filter out headlines with certain URLs (adjust this condition as needed)\n",
    "        if \"/news/\" in headline['href']:\n",
    "            headlines_text.append(headline.text.strip())\n",
    "\n",
    "    # Remove the last 52 rows\n",
    "    if len(headlines_text) > 52:\n",
    "        headlines_text = headlines_text[:-52]\n",
    "\n",
    "    # Extract date from the URL\n",
    "    url_date_str = url.split('starttime-')[1].split('.')[0]\n",
    "    url_date = datetime(1899, 12, 30) + timedelta(days=int(url_date_str))\n",
    "\n",
    "    # Write the headlines and corresponding dates to the Excel file\n",
    "    for headline in headlines_text:\n",
    "        ws.append([headline, url_date.strftime(\"%d/%b/%Y\")])\n",
    "\n",
    "# Iterate through each starttime value from 43466 to 43496 and fetch data\n",
    "for starttime in range(43466, 43497):\n",
    "    url = f\"https://economictimes.indiatimes.com/archivelist/year-2019,month-5,starttime-{starttime}.cms\"\n",
    "    extract_headlines(url, ws)\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(file_path)\n",
    "\n",
    "print(\"Data has been successfully written to\", file_path)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8329abee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully written to D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import load_workbook\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize the Selenium webdriver (make sure you have chromedriver installed and its path set)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "file_path = r\"D:\\MIT PGDAML\\Neural Network\\Project\\ET_Archives.xlsx\"\n",
    "try:\n",
    "    wb = load_workbook(filename=file_path)\n",
    "    ws = wb.active\n",
    "except FileNotFoundError:\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Headline\", \"Date\"])  # Add column headers\n",
    "\n",
    "# Function to extract headlines and corresponding dates from a given URL\n",
    "def extract_headlines(url, ws):\n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Find all the <a> tags containing headlines\n",
    "    headlines = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    # Extract and write the text of each headline along with the corresponding date\n",
    "    headlines_text = []\n",
    "    for headline in headlines:\n",
    "        # Filter out headlines with certain URLs (adjust this condition as needed)\n",
    "        if \"/news/\" in headline['href']:\n",
    "            headlines_text.append(headline.text.strip())\n",
    "\n",
    "    # Remove the last 52 rows\n",
    "    if len(headlines_text) > 52:\n",
    "        headlines_text = headlines_text[:-52]\n",
    "\n",
    "    # Extract date from the URL\n",
    "    url_date_str = url.split('starttime-')[1].split('.')[0]\n",
    "    url_date = datetime(1899, 12, 30) + timedelta(days=int(url_date_str))\n",
    "\n",
    "    # Write the headlines and corresponding dates to the Excel file\n",
    "    for headline in headlines_text:\n",
    "        ws.append([headline, url_date.strftime(\"%d/%b/%Y\")])\n",
    "\n",
    "# Iterate through each starttime value from 43831 to 43861 and fetch data#mar2020\n",
    "for starttime in range(45261, 45323):\n",
    "    url = f\"https://economictimes.indiatimes.com/archivelist/year-2019,month-5,starttime-{starttime}.cms\"\n",
    "    extract_headlines(url, ws)\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(file_path)\n",
    "\n",
    "print(\"Data has been successfully written to\", file_path)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5b2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
